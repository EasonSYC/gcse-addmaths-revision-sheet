\documentclass[8pt]{article}
\usepackage{allan-eason}

\usepackage{siunitx}
\usepackage{accents}

\usepackage[english]{babel}

\setcounter{tocdepth}{1}

\theoremstyle{remark}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\newtheorem*{remark}{Remark}

\usetikzlibrary{positioning}
\usetikzlibrary{svg.path}

\graphicspath{ {./images/} }


\newcommand{\Date}{\today}
\newcommand{\Name}{GCSE Maths Knowledge Sheet}
\newcommand{\Subname}{Eason's Mathematics Toolbox}
\newcommand{\Title}{\Name\\ \Large{\Subname}}

\newcommand{\Author}{Yicheng Shao}

\author{\Author}
\title{\Title}
\date{Version 1. \Date}

\lhead{\Name}

\begin{document}

	\maketitle

	\tableofcontents

    \section*{What is this and why this?}

        Mathematics is one of my favourite subjects and it is very important whatever you are doing in the future. It is mostly about techniques for solving problems, but the knowledge behind all those techniques is vital for understanding. To aid practice, I produced this document based on the syllabus.

        This is more of an extension of the syllabus and the structure is the same. However, it provides some sample answers for those questions in the syllabus and is a good way to refer to your self-assessment based on the syllabus.

        I am also an IGCSE student so errors are inevitable in this document. Feel free to email \href{eason.syc@icloud.com}{eason.syc@icloud.com} to point out any mistakes or submit an issue on the GitHub page!

        This document assumes prior knowledge of CIE IGCSE Mathematics.

    \setcounter{section}{-1}
    
    \section{Prior Knowledge and Notations}
        \subsection{Set Notations}

            \begin{definition}[set and elements]
                If $x$ is an element of set $S$, we denote $x \in S$. Otherwise, $x \notin S$.
            \end{definition}

            \begin{definition}[set constructors]
                We have two set construction notations:
                $$
                    \{x \mid P(x)\} = \{x : P(x)\}
                $$
                defines a set containing all $x$ satisfying condition $P(x)$.

                $$
                    \{x_1, x_2, \ldots\}
                $$
                defines a set with elements $x_1, x_2, \ldots$.
            \end{definition}

            \begin{definition}[empty set, universal set]
                We use $\emptyset$ or $\varnothing$ to define the empty set (set with no elements) and use $\mathcal{E}$ to denote the universal set.
            \end{definition}

            \begin{definition}[cardinality]
                We use $n(S)$ to denote the number of elements in set $S$.
            \end{definition}

            \begin{definition}[complement]
                We use $S'$ to define the complement of set $S$,
                $$
                    S' = \{x \in \mathcal{E} \mid x \notin S\}
                $$
            \end{definition}

            \begin{definition}[subset, proper subset]
                We denote 
                $$A \subseteq B$$
                if $x \in A \implies x \in B$.

                Furthermore, if $A \neq B$, we denote it as
                $$A \subset B.$$
            \end{definition}

            \begin{definition}[union, intersection]
                We denote
                $$
                    A \cap B = \{x \mid x \in A \lgand x \in B\},
                $$
                and
                $$
                    A \cup B = \{x \mid x \in A \lgor x \in B\}.
                $$
            \end{definition}

            \begin{definition}[number sets]
                The set $\NN$ is the natural numbers, $\NN = \{ 1, 2, 3, \ldots \}$.

                The set $\ZZ$ is the integers, $\ZZ = \{\ldots, -3, -2, -1, 0, 1, 2, 3, \ldots\}$.

                The set $\QQ$ is the rational numbers,
                $$
                    \QQ = \left\{\frac{p}{q} \mid p, q \in \ZZ, q \neq 0\right\}.
                $$

                The set $\RR$ is the set of real numbers.
            \end{definition}

            \begin{definition}[intervals]
                We define the intervals as follows:
                \begin{align*}
                    (a, b) &= \{x \mid a < x < b\},\\
                    (a, b] &= \{x \mid a < x \leq b\},\\
                    [a, b) &= \{x \mid a \leq x < b\},\\
                    [a, b] &= \{x \mid a \leq x \leq b\},\\
                    (a, +\infty) &= \{x \mid x > a\},\\
                    [a, +\infty) &= \{x \mid x \geq a\},\\
                    (-\infty, b) &= \{x \mid x < b\},\\
                    (-\infty, b] &= \{x \mid x \leq b\},\\
                    (-\infty, +\infty) &= \RR.
                \end{align*}
            \end{definition}

        \subsection{Relationship Symbols and Operations}
            \begin{definition}[implies, implied by, equivilent]
                $A$ implies $B$ is denoted by $A \implies B$, $B$ implies $A$ is denoted by $A \impliedby B$, $A$ and $B$ are equivilent is denoted by $A \iff B$.
            \end{definition}

            \begin{definition}[sum and product]
                We define
                $$
                    \sum_{i = 1}^{n} a_i = a_1 + a_2 + \ldots + a_n,
                $$
                and
                $$
                    \prod_{i = 1}^{n} a_i = a_1 \cdot a_2 \cdots a_n.
                $$
            \end{definition}

            \begin{definition}[binominal coefficient and factorial]
                We define
                $$
                    n! = n \cdot (n - 1) \cdots 1,
                $$
                with $0! = 1$, hence defining
                $$
                    \binom{n}{r} = \frac{n!}{r!(n-r)!}
                $$
            \end{definition}
        
        \subsection{Functions}
            \begin{definition}[composite of two functions]
                We define
                $$
                    gf(x) = g(f(x)).
                $$
            \end{definition}

            \begin{definition}[derivative]
                We denote
                $$
                    \frac{\diff^n f(x)}{\diff x^n} = f^{(n)} (x)
                $$
                as the $n$th derivative of $f(x)$.
            \end{definition}

        \subsection{Triangles}
            \begin{theorem}[sine rule, cosine rule, area]
                In $\triangle ABC$ with side lengths $a, b, c$ and angles $A, B, C$, we have
                $$
                    \frac{a}{\sin A} = \frac{b}{\sin B} = \frac{c}{\sin C},
                $$
                $$
                    a^2 = b^2 + c^2 - 2bc \cos A,
                $$
                $$
                    b^2 = a^2 + c^2 - 2ac \cos B,
                $$
                $$
                    c^2 = a^2 + b^2 - 2ab \cos C,
                $$
                $$
                    \text{area}=\frac{1}{2} ab \sin C = \frac{1}{2} bc \sin A = \frac{1}{2} ac \sin B.
                $$
            \end{theorem}

    \section{Functions}

        \begin{definition}[function, domain, image]
            A \textbf{fucntion} $f: A \rightarrow B$ is defined as a mapping which maps each element in $A$ to exactly one element in $B$. Effectively, a function is an operation on a thing which produces another thing.

            We call $A$ the \textbf{domain} (the function can operate on this set). (And $B$ the co-domain.)

            We define the set
            $$
                \{f(x) \mid x \in A\}
            $$
            as the \textbf{range} of the function, which is all the outputs of the function.

            At this stage, $B$ will be $\RR$ and $A$ will be a subset of $\RR$.
        \end{definition}

        \begin{definition}[one-to-one, many-to-one]
            We call a function $f$ \textbf{one-to-one}, or injective, when
            $$
                f(x_1) = f(x_2) \implies x_1 = x_2.
            $$
            
            This means that each output a function will produce can only appear by operating on exactly one element.

            If a function is not one-to-one, we call it \textbf{many-to-one}.
        \end{definition}

        \begin{definition}[function notations]
            The result that $f$ maps an element of the domain $x$ to is denoted as $f(x)$. As an example, if function $f$ maps $x$ to $\sin x$, then the following are equivalent:
            \begin{enumerate}
                \item $f(x) = \sin x$,
                \item $f: x \mapsto \sin x$.
            \end{enumerate}
        \end{definition}

        \begin{definition}[inverse]
            A function's inverse, denoted as $f^{-1}(x)$, is defined from the range of $f(x)$ to the domain of $f(x)$, and satisfies that:
            $$
                f^{-1}(f(x)) = f(f^{-1}(x))= x.
            $$
        \end{definition}

        \begin{theorem}[unique inverse]
            If the inverse of a function exists, then it is unique.
        \end{theorem}

        \begin{theorem}[condition for existence of inverse]
            The inverse of a function exists if and only if it is one-to-one.
        \end{theorem}

        \begin{remark}
            The previous theorem is true if and only if the inverse is defined from the range. If the inverse is defined from the co-domain then we also require the function to surjective (i.e. range equals to co-domain) hence bijective. This is a very useful concept (isomorphism)!
        \end{remark}

        \begin{theorem}[inverse graphs]
            The graph of the inverse of a function and the function itself is symmetric by the line $y = x$.
        \end{theorem}

        \begin{definition}[composite]
            The composite of $f$ with $f$ denoted as $f^2$ is defined as follows:
            $$
            f^2(x) = f(f(x)).
            $$
        \end{definition}

        \begin{definition}[modulus]
            The graph of $\abs{f(x)}$ and $f(x)$ has a relationship as follows:

            The graph of $\abs{f(x)}$ reflects the part of the graph of $f(x)$ below the $x$ axis with regards to the $x$ axis (basically flip it up).
        \end{definition}

    \section{Quadratic}
        \begin{definition}[quadratic]
            A quadratic function $f$ is defined as an element of $\PP[x]$ where $\deg f(x) = 2$.

            Just kidding. A quadratic function $f$ is defined as
            $$
            f(x) = ax^2 + bx + c
            $$
            where $a \neq 0$.
        \end{definition}

        \begin{theorem}[extremum property]
            A quadratic function $f(x)$ has a maximum if and only if $a < 0$, and it has a minimum if and only if $a > 0$. The turning point (extremum point in this case) of a quadratic is
            $$
                \left(-\frac{b}{2a}, \frac{4ac - b^2}{4ac}\right).
            $$

            \begin{proof}
                We can show this by \textbf{completing the square}.
                \begin{align*}
                    ax^2 + bx + c &= a \left(x^2 + \frac{b}{a} x\right) + c\\
                    &= a \left(x^2 + 2 \cdot \frac{b}{2a} \cdot x \right) + c\\
                    &= a \left[x^2 + 2 \cdot \frac{b}{2a} \cdot x + \left(\frac{b}{2a}\right)^2\right] - a \cdot \left(\frac{b}{2a}\right)^2 + c\\
                    &= a \cdot \left(x + \frac{b}{2a}\right)^2 - \frac{b^2}{4a} + c.
                \end{align*}

                If $a > 0$, then we have
                \begin{align*}
                    ax^2 + bx + c \geq -\frac{b^2}{4a} + c,
                \end{align*}
                where the equal sign holds if and onlly if $x = - \frac{b}{2a}$.

                A similar argument holds for $a < 0$.
            \end{proof}

            \begin{proof}
                We can also show this by \textbf{differentiation}.
            \end{proof}
        \end{theorem}

        \begin{theorem}[roots]
            The roots (solutions) to the quadratic will be
            $$
                x_{1, 2} = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}.
            $$
        \end{theorem}

        \begin{theorem}[discriminant]
            The \textbf{discriminant} for the quadratic $ax^2 + bx + c$ is defined as
            $$
                \Delta = b^2 - 4ac.
            $$

            When $\Delta > 0$, the quadratic has two distinct real roots; when $\Delta = 0$, the quadratic has two equal real roots; when $\Delta < 0$, the quadratic has two complex roots which are complex conjugate of each other (i.e. sum to a real number).
        \end{theorem}

        \begin{theorem}[quadratic and a line]
            The intersections for a quadratic and a line (which is not perpendicular to the $x$ axis) can be found by equating their equations and solving the corresponding equation (which is a quadratic).
        \end{theorem}

        \begin{theorem}[quadratic inequalities]
            A quadratic inequality can be solved by finding the two solutions (known as \textbf{critical values}).

            For the quadratic $f(x) = ax^2 + bx + c$ where $a > 0$ ($a < 0$ can be considered similarly),
            \begin{enumerate}
                \item $\Delta > 0$. Let the two roots be $x_1$ and $x_2$.
                
                The solution set to $f(x) > 0$ is
                $$
                    (-\infty, x_1) \cup (x_2, +\infty).
                $$

                The solution set to $f(x) < 0$ is
                $$
                    (x_1, x_2).
                $$

                \item $\Delta = 0$. Let the root be $x_r$.
                
                The solution set to $f(x) > 0$ is
                $$
                    (-\infty, x_r) \cup (x_r, +\infty).
                $$

                The solution set to $f(x) < 0$ is $\emptyset$.

                \item $\Delta < 0$. The solution set to $f(x) > 0$ is $\RR$ and the solution set to $f(x) < 0$ is $\emptyset$.
            \end{enumerate}
        \end{theorem}

    \section{Equations, inequalities and graphs}
        \begin{theorem}[type $\abs{ax + b} = c, a \neq 0, c \geq 0$]
            The solutions to the equation
            $$
                \abs{ax + b} = c
            $$
            is
            $$
                x_1 = \frac{c - b}{a}, x_2 = \frac{-c -b}{a}.
            $$

            \begin{proof}
                The solution to this can be shown by dividing it into cases, where $ax + b = -c$ or $ax + b = c$.
            \end{proof}

            \begin{proof}
                This solution can also be shown by squaring both sides to get rid of the absolute value and using quadratic solving methods. I would not prefer it in the first place.
            \end{proof}
        \end{theorem}

        \begin{theorem}[generalise: $\abs{f(x)} = c \geq 0$]
            The solution to this equation is the same as the solutions to $f(x) = \pm c$.
        \end{theorem}

        \begin{theorem}[type $\abs{ax + b} = \abs{cx + d}$]
            The solutions to the equation
            $$
            \abs{ax + b} = \abs{cx + d}
            $$
            is
            $$
            x_{1, 2} = \frac{(cd - ab) \pm 2 (ad - bc)}{a^2 - c^2}.
            $$

            \begin{proof}
                We can show it by squaring both sides getting
                $$
                (ax + b)^2 = (cx + d)^2 \implies (a^2 - c^2)x^2 + 2(ab - cd) x + (b^2 - d^2) = 0.
                $$

                The discriminant will be
                \begin{align*}
                    \Delta &= [2(ab - cd)]^2 - 4 (a^2 - c^2) (b^2 - d^2)\\
                    &= 4 [(a^2 b^2 - 2abcd + c^2 d^2) - a^2b^2 - c^2d^2 + a^2 d^2 + b^2 c^2]\\
                    &= 4 [a^2 d^2 - 2abcd + b^2 c^2]\\
                    &= 4 (ad - bc)^2.
                \end{align*}

                Hence, solutions will be
                \begin{align*}
                    x_{1, 2} &= \frac{- 2 (ab - cd) \pm \sqrt{\Delta}}{2 (a^2 - c^2)}\\
                    &= \frac{(cd - ab) \pm 2 (ad - bc)}{a^2 - c^2}.
                \end{align*}
            \end{proof}

            \begin{proof}
                We can also show this by considering the order relationship between $-\frac{b}{a}, - \frac{d}{c}, x$ and expanding absolute values. I would not prefer it in the first place.
            \end{proof}
        \end{theorem}

        \begin{theorem}[type $\abs{f(x)} = \abs{g(x)}$]
            This solution will be the same as $[f(x)]^2 = [g(x)]^2$.
        \end{theorem}

        \begin{theorem}[type $\abs{ax + b} > / \leq c, c \geq 0$]
            Solutions to $\abs{ax + b} > c$ with previously mentioned restrictions will be the same as the solution to
            $$
                (ax + b)^2 - c^2 > 0.
            $$

            I would prefer to square everything if this is an inequality, but we should be careful whether this operation is equivalent or not. (Will it introduce more solutions? Will it ignore certain solutions?)
        \end{theorem}

        \begin{theorem}[type $\abs{ax + b} \leq \abs{cx + d}$]
            Solutions to this will be equivalent to
            $$
            (a^2 - c^2) x^2 + 2(ab - cd)x + (b^2 - d^2) \leq 0.
            $$

            We can also do this by considering the relationship between $x, -\frac{b}{a}, -\frac{d}{c}$, but I think this way is easier.
        \end{theorem}

        \begin{theorem}[graph of $p(x) = k (x-a) (x-b) (x-c), k \neq 0$]
            The graph of $p(x)$ will satisfies the follows:
            $$
                \lim_{x \rightarrow +\infty} p(x) = - \lim_{x \rightarrow -\infty} p(x) = +\infty (\text{if $k$ > 0}), = -\infty (\text{if $k$ < 0}),
            $$
            which shows the trends of the graph of $p(x)$ when it approaches infinity (go further to the left/right) and will have the same symbol as $k$.

            Furthermore, the intersections of $p(x)$ and $x$-axis will be $ a, b, c$, since the intersection with $x$-axis implies $p(x) = 0$ (and they are the only ones due to the fundamental theorem of algebra).
        \end{theorem}

    \section{Indices and surds}
        In this section, we will recall the full definition of the power of a positive number.

        \begin{definition}[$a^b$ where $a > 0, b \in \NN$]
            We inductively define it by the base case
            $$a^0 = 1,$$
            and $$a^b = a^{b - 1} \cdot a\text{ for } b \geq 1,$$
            in the positive direction, and
            $$a^b = a^{b + 1} \cdot \frac{1}{a}\text{ for } b \leq -1$$
            in the negative direction.
        \end{definition}

        \begin{definition}[$a^b$ where $a > 0, b = \frac{p}{q}, p \in \ZZ, q \in \NN$]
            We define it as
            $$
                a^{b} = a^{\frac{p}{q}} = \sqrt[q]{a^p}.
            $$
        \end{definition}

        \begin{theorem}[calculation properties of powers]
            We have
            \begin{align*}
                a^b \cdot a^c &= a^{b + c},\\
                \frac{a^b}{a^c} &= a^{b - c},\\
                (a^b)^c = (a^c)^b &= a^{bc},\\
                (ab)^c &= a^c b^c,\\
                \left(\frac{a}{b}\right)^c &= \frac{a^c}{b^c}\\
                a^0 &= 1,\\
                a^{-n} &= \frac{1}{a^n},\\
                a^{\frac{1}{n}} &= \sqrt[n]{a},\\
                a^{\frac{m}{n}} &= \left(\sqrt[n]{a}\right)^m = \sqrt[n]{a^m}.
            \end{align*}

            Readers should verify that the previous two definitions are well-defined under those properties (because I believe those properties are the reason why we define them as previously defined).
        \end{theorem}

        \begin{theorem}[calculation properties of roots]
            We have
            \begin{align*}
                \sqrt{ab} &= \sqrt{a} \cdot \sqrt{b},\\
                \sqrt{\frac{a}{b}} &= \frac{\sqrt{a}}{\sqrt{b}},\\
                \sqrt{a} \cdot \sqrt{a} &= a.
            \end{align*}
        \end{theorem}

        For $ b \in \QQ'$ (i.e. $b$ is an irrational number), we define it by a limit of rational numbers (Cauchy Sequence).

        To rationalise a fraction, we use the following theorem (in fact, a method, by conjugate roots)
        
        \begin{theorem}[rationalising denominator]
            We have
            $$
                \frac{k}{\sqrt{a}} = \frac{k \sqrt{a}}{a},
            $$
            $$
                \frac{k}{\sqrt{a} - \sqrt{b}} = \frac{k(\sqrt{a} + \sqrt{b})}{a - b},
            $$
            $$
                \frac{k}{\sqrt{a} + \sqrt{b}} = \frac{k(\sqrt{a} - \sqrt{b})}{a - b}.
            $$
        \end{theorem}

    \section{Factors of polynomials}
        \begin{definition}[polynomial]
            A polynomial is an element of the linear space/communicative ring $\PP [x]$.

            Just kidding. A \textbf{polynomial} $p(x)$ can be expressed as a sum
            $$
                p(x) = \sum_{i = 0}^{n} k_i x^i
            $$
            for some non-negative integer $n$ which we call the degree (DANGER ZONE! degree of the polynomial 0 is negative infinity), and real number $k_i$s which are not all 0. 
        \end{definition}

        \begin{definition}[root]
            A \textbf{root} $x_0$ of a polynomial $p(x)$ satisfies that $p(x_0) = 0$.
        \end{definition}

        \begin{theorem}[factor theorem]
            $x_0$ is a root of $p(x)$ if and only if $(x - x_0)$ is a factor of $p(x)$.

            \begin{proof}
                By basic properties of division, let $p(x) = q(x) (x - x_0) + r(x)$.

                If $x_0$ is a root of $p(x)$, then by definition we have $p(x_0) = r(x) = 0$ which means $p(x)$ has a remainder of $0$ upon division by $x - x_0$.
                
                If $x - x_0$ is a factor of $p(x)$, we have $r(x) = 0$ and $p(x) = q(x) (x - x_0)$. Plugging in $x = x_0$ will give us $p(x) = 0$ hence $x_0$ is a root of $p(x)$.
            \end{proof}
        \end{theorem}

        \begin{theorem}[remainder theorem]
            The remainder of $p(x)$ divided by $(x - x_0)$ will be equal to $p(x_0)$.

            \begin{proof}
                The proof is similar to the previous one. The reader should verify so.
            \end{proof}
        \end{theorem}

    \section{Simultaneous equations}
        
        This section does not have a lot to do.

        Two ways of solving simultaneous equations are \textbf{elimination} or \textbf{substitution}.

        There is an advanced way of dealing with Linear Equations (i.e. unknown maximum power of 1) by using matrices, ranks (linear algebra) and Gaussian Elimination. But it's just elimination, using more advanced ways to express so.
    
    \section{Logarithmic and exponential functions}
        \begin{definition}[exponential functions]
            An exponential function $f(x)$ has the form of follows
            $$
                f(x) = a^x
            $$
            where $a > 0$ and $a \neq 1$.

            Exponential functions are defined on $\RR$ and have a range of $(0, +\infty)$.
        \end{definition}

        \begin{definition}[logarithm]
            We define the function $\log_a x$ as the inverse of $a^x$ where $a > 0$, $a \neq 1$. In fact,
            $$
                y = a^x \iff x = \log_a y, a > 0, a \neq 1.
            $$
        \end{definition}

        \begin{definition}[logarithm functions]
            A logarithmic function $f(x)$ has the form of follows
            $$
                f(x) = \log_a x
            $$ 
            wherer $a > 0$ and $a \neq 1$.

            In the case of $a = e$, we denote it as
            $$
                f(x) = \ln x,
            $$
            and in the case of $a = 10$, we denote it as
            $$
                f(x) = \lg x.
            $$

            Logarithmic functions are defined on $(0, +\infty)$ and have a range of $\RR$.
        \end{definition}
    
        \begin{theorem}[logarithm calculation basics]
            We have
            $$
                \log_a a = 1, \log_a 1 = 0, \log_a a^x = x, a^{\log_a x} = x.
            $$
        \end{theorem}

        \begin{theorem}[logarithm calculation rules]
            We have
            \begin{align*}
                \log_a (xy) &= \log_a x + \log_a y, \\
                \log_a \left(\frac{x}{y}\right) &= \log_a x - \log_a y,\\
                \log_a (x^m) &= m \log_a x.
            \end{align*}
        \end{theorem}

        \begin{corollary}
            $$
            \log_a \left(\frac{1}{x}\right) = - \log_a x.
            $$
        \end{corollary}

        \begin{theorem}[change of base]
            We have
            $$
                \log_b a = \frac{\log_c a}{\log_c b}.
            $$
        \end{theorem}
        
        \begin{corollary}
            $$
                \log_b a = \frac{1}{\log_a b}.
            $$
        \end{corollary}

        \begin{theorem}[graphs of exponentials]
            For a exponential $a^x$ where $a > 1$, we have
            $$
                \lim_{x \rightarrow +\infty} a^x = +\infty, \lim_{x \rightarrow -\infty} a^x = 0.
            $$

            For a exponential $a^x$ where $a < 1$, we have
            $$
                \lim_{x \rightarrow +\infty} a^x = 0, \lim_{x \rightarrow -\infty} a^x = +\infty.
            $$
        \end{theorem}

        \begin{theorem}[graphs of logarithms]
            For a logarithm $\log_a x$ where $a > 1$, we have
            $$
                \lim_{x \rightarrow 0^+} \log_a x = -\infty, \lim_{x \rightarrow +\infty} \log_a x = +\infty.
            $$ 

            For a logarithm $\log_a x$ where $a < 1$, we have
            $$
                \lim_{x \rightarrow 0^+} \log_a x = +\infty, \lim_{x \rightarrow +\infty} \log_a x = -\infty.
            $$
        \end{theorem}

    \section{Straight line graphs}
        \begin{definition}[straight line]
            A \textbf{straight line} is an equation of the form $y = mx + c$, where $m$ is the \textbf{gradient} and $c$ is the \textbf{$y$-interception}.
        \end{definition}

        \begin{definition}[expression forms]
            Other forms of expressing lines include
            $$
                ax + by + c = 0,
            $$
            and when we know a gradient and a point, we can use
            $$
                (y - y_0) = m(x - x_0).
            $$
        \end{definition}

        \begin{theorem}[mid-point]
            The mid-point of a ling segment with points $A(x_1, y_1)$ and $B(x_2, y_2)$ is
            $$
                M\left(\frac{x_1 + x_2}{2}, \frac{y_1 + y_2}{2}\right).
            $$
        \end{theorem}

        \begin{theorem}[length/distance]
            The distance between two points $A(x_1, y_1)$ and $B(x_2, y_2)$ is
            $$
                \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}.
            $$
        \end{theorem}

        \begin{theorem}[parallel condition]
            Two lines $l_1: y = m_1 x + c_1$ and $l_2: y = m_2 x + c_2$ are parallel if and only if $m_1 = m_2$.
        \end{theorem}

        \begin{theorem}[perpendicular condition]
            Two lines $l_!: y = m_1 x + c_1$ and $l_2: y = m_2 x + c_2$ are perpendicular if and only if $m_1 m_2 = -1$.
        \end{theorem}

        \begin{theorem}[area of a triangle]
            The area of a triangle of three vertices $A(x_1, y_1), B(x_2, y_2), C(x_3, y_3)$ is equal to
            $$
                S = \frac{1}{2} \abs{\begin{vmatrix}
                    1 & 1 & 1\\
                    x_1 & x_2 & x_3\\
                    y_1 & y_2 & y_3
                \end{vmatrix}}
                = \frac{1}{2} \abs{x_1 y_2 + x_2 y_3 + x_3 y_1 - x_2 y_1 - x_3 y_2 - x_1 y_3}.
            $$
        \end{theorem}

    \section{Circular measure}
        \begin{definition}[radians]
            In a circle with a radius of $1$, \textbf{$1$ radian} is defined as the angle at the centre when it has a corresponding arc length of $1$.
        \end{definition}

        \begin{corollary}
            $$
                \pi = 180\degree.
            $$
        \end{corollary}

        \begin{corollary}
            Multiply $\frac{180}{\pi}$ to convert degrees to radians.
            
            Multiply $\frac{\pi}{180}$ to convert radians to degrees.
        \end{corollary}

        \begin{corollary}[arc length]
            $$l = r \theta$$
            where $l$ is arc length, $r$ is radius and $\theta$ is the angle at the centre.
        \end{corollary}

        \begin{corollary}[sector area]
            $$A = \frac{1}{2} r^2 \theta$$
            where $A$ is area of sector, $r$ is radius and $\theta$ is the angle at the center.
        \end{corollary}

    \section{Trigonometry}
        
        \begin{definition}[trig functions, prior knowledge]
            In a right-angle triangle with hypotenuse $r$, angle $\theta$, oppopsite edge $y$ and adjacent edge $x$, we define
            $$
                \sin \theta = \frac{y}{r}, \cos \theta = \frac{x}{r}, \tan \theta = \frac{y}{x}.
            $$
        \end{definition}

        \begin{theorem}[value of important angles]
            $$\sin 0 = 0, \cos 0 = 1, \tan 0 = 0,$$
            $$\sin \frac{\pi}{6} = \frac{1}{2}, \cos \frac{\pi}{6} = \frac{\sqrt{3}}{2}, \tan \frac{\pi}{6} = \frac{1}{\sqrt{3}},$$
            $$\sin \frac{\pi}{4} = \frac{\sqrt{2}}{2}, \cos \frac{\pi}{4} = \frac{\sqrt{2}}{2}, \tan \frac{\pi}{4} = 1,$$
            $$\sin \frac{\pi}{3} = \frac{\sqrt{3}}{2}, \cos \frac{\pi}{3} = \frac{1}{2}, \tan \frac{\pi}{3} = \sqrt{3},$$
            $$\sin \frac{\pi}{2} = 1, \cos \frac{\pi}{2} = 0, \tan \frac{\pi}{2} = \infty.$$
        \end{theorem}

        \begin{definition}[angle]
            An \textbf{angle} is a measure of the rotation of a line $OP$ around $O(0, 0)$ from the positive $x$-direction, with anti-clockwise taken as the positive angle and clockwise as negative.
        \end{definition}

        \begin{definition}[trig functions]
            Trigonometric ratios of any angle $\theta$ can be defined as
            $$
                \sin \theta = \frac{y}{r}, \cos \theta = \frac{x}{r}, \tan \theta = \frac{y}{x},
            $$
            where $P(x, y)$ and $r = OP = \sqrt{x^2 + y^2}$.
        \end{definition}

        \begin{theorem}
            All trig functions are positive in the first quadrant. Only $\sin$ is positive in the second quadrant. Only $\tan$ is positive in the third quadrant. Only $\cos$ is positive in the fourth quadrant.
        \end{theorem}

        \newcommand{\cosec}{\mathrm{cosec}}

        \begin{definition}[$\cot, \sec, \cosec$]
            We define the three extra trig functions as follows:
            $$
                \cot \theta = \frac{x}{y}, \sec \theta = \frac{r}{x}, \cosec \theta = \frac{r}{y}.
            $$
        \end{definition}

        \begin{theorem}[trig identities]
            The following trig identities are very basic and important:
            $$
                \cosec \theta = \frac{1}{\sin \theta}, \sec \theta = \frac{1}{\cos \theta}, \cot \theta = \frac{1}{\tan \theta},
            $$
            $$
                \sin^2 \theta + \cos^2 \theta = 1, 1 + \tan^2 \theta = \sec^2 \theta, \cot^2 \theta + 1 = \cosec^2 \theta,
            $$
            $$
                \tan \theta = \frac{\sin \theta}{\cos \theta}, \cot \theta = \frac{\cos \theta}{\sin \theta}.
            $$

            In this case, I feel I have to make the following clear while dealing with $\infty$ of $\tan, \cot, \sec, \cosec$. We treat $0 \cdot \infty = 1, 0 = \frac{1}{\infty}, \infty = \frac{1}{0}$. DANGER! Informal use of notation.
        \end{theorem}

        \begin{definition}[periodic function]
            For a function $f: A \rightarrow B$, if for a certain real number $T$, we have $x \in A \implies x + T \in A$, and
            $$f(x) = f(x + T)$$
            for all $x \in A$, we say $T$ is a \textbf{period} of the function $f$. We say $T$ is a \textbf{minimum positive period} if $T$ is the smallest positive number which is a period of that function.
        \end{definition}

        \begin{theorem}[trig as functions]
            The function $y = \sin x$ has a domain of $\RR$ and a range of $[-1, 1]$. It has a minimum positive period of $2\pi$, and an amplitude of $1$. Restricting its domain onto $[-\frac{\pi}{2}, \frac{\pi}{2}]$, its inverse, defined from $[-1, 1]$ to $[-\frac{\pi}{2}, \frac{\pi}{2}]$ is denoted as $\arcsin x$.

            The function $y = \cos x$ has a domain of $\RR$ and a range of $[-1, 1]$. It has a minimum positive period of $2\pi$, and an amplitude of $1$. Restricting its domain to $[0, \pi]$, its inverse, defined from $[-1, 1]$ onto $[0, \pi]$ is denoted as $\arccos x$.

            The function $y = \tan x$ has a domain of $\RR \setminus \{\frac{\pi}{2} + k\pi \mid k \in \ZZ\}$ (all real numbers except for $\{\ldots, -\frac{\pi}{2}, \frac{\pi}{2}, \frac{3\pi}{2}, \ldots\}$), and a range of $\RR$. It has a minimum positive period of $\pi$. Restricting its domain to $(-\frac{\pi}{2}, \frac{\pi}{2})$, its inverse, defined from $\RR$ onto $(-\frac{\pi}{2}, \frac{\pi}{2})$ is denoted as $\arctan x$.

            Notice that since $\forall k \in \ZZ$, we have
            $$
                \lim_{x \rightarrow k\pi + \frac{\pi}{2}} \tan x = \infty,
            $$
            we know that the lines $x = k\pi + \frac{\pi}{2}, k \in \ZZ$ are the vertical asymptotes for $y = \tan x$.

            The function $y = \cot x$ has a domain of $\RR \setminus \{k\pi \mid k \in \ZZ\}$ (all real numbers except for $\{\ldots, -\pi, 0, \pi, 2\pi, \ldots\}$), and a range of $\RR$. It has a minimum positive period of $\pi$.

            Notice that since $\forall k \in \ZZ$, we have
            $$
                \lim_{x \rightarrow k\pi} \cot x = \infty,
            $$
            we know that the lines $x = k\pi, k \in \ZZ$ are the vertical asymptotes for $y = \cot x$.

            The function $y = \sec x$ has a domain of $\RR \setminus \{\frac{\pi}{2} + k\pi \mid k \in \ZZ\}$ and a range of $(-\infty, -1] \cup [1, +\infty)$. It has a minimum positive period of $2\pi$.

            The function $y = \csc x$ has a domain of $\RR \setminus \{k\pi \mid k \in \ZZ\}$ and a range of $(-\infty, -1] \cup [1, +\infty)$. It has a minimum positive period of $2\pi$.
        \end{theorem}

        \begin{theorem}[trig function manipulations]
            The function $y = a \sin bx + c$ has an amplitude of $a$, a period of $\frac{2\pi}{b}$ and is translated upwards by $c$ units.

            The function $y = a \cos bx + c$ has an amplitude of $a$, a period of $\frac{2\pi}{b}$ and is translated upwards by $c$ units.

            The function $y = a \tan bx + c$ stretches the graph vertically by a factor of $a$, has a period of $\frac{\pi}{b}$ and is translated upwards by $c$ units.
        \end{theorem}

    \section{Differentiation and integration}
        \subsection{Differentiation}

            \begin{definition}[derivative]
                We define the \textbf{gradient} or \textbf{derivative} of a function $f: A \rightarrow B$ at a point $x_0$, given that it is defined at a neighbourhood of $x_0$ (i.e. $\exists \delta, (x_0 - \delta, x_0 + \delta) \subseteq A$), is equal to
                $$
                    \lim_{x \rightarrow x_0} \frac{f(x) - f(x_0)}{x - x_0}.
                $$
                We will denote the previous value as
                $$
                    f'(x_0), \at{\frac{\diff f(x)}{\diff x}}{x = x_0}, \at{\frac{\diff}{\diff x} f(x)}{x = x_0}.
                $$

                The \textbf{derivative function} or \textbf{gradient function} of $f(x)$ is denoted as
                $$
                    f'(x), \frac{\diff f(x)}{\diff x}, \frac{\diff}{\diff x} f(x).
                $$
            \end{definition}

            \begin{theorem}[linearality of differential operator]
                Given functions $f(x)$ and $g(x)$ and a constant $k$, we have
                $$
                    \frac{\diff}{\diff x} (f(x) + g(x)) = \frac{\diff}{\diff x} f(x) + \frac{\diff}{\diff x} g(x),
                $$
                $$
                    \frac{\diff}{\diff x} (k f(x)) = k\frac{\diff}{\diff x} f(x).
                $$
            \end{theorem}

            \begin{theorem}[chain rule]
                The \textbf{chain rule} states that
                $$
                    \frac{\diff y}{\diff x} = \frac{\diff y}{\diff u} \cdot \frac{\diff u}{\diff x}.
                $$
            \end{theorem}

            \begin{theorem}[product rule]
                We have
                $$
                    \frac{\diff}{\diff x} (uv) = u \frac{\diff}{\diff x} v + v \frac{\diff}{\diff x} u.
                $$
            \end{theorem}

            \begin{theorem}[quotient rule]
                We have
                $$
                    \frac{\diff}{\diff x}\left(\frac{u}{v}\right) = \frac{v \frac{\diff u}{\diff x} - u \frac{\diff v}{\diff x}}{v^2}.
                $$
            \end{theorem}

            \begin{definition}[tangents and normals]
                For a function $y = f(x)$, the \textbf{gradient} at the point $P(x_0, f(x_0))$ is equal to $f'(x_0)$.

                The \textbf{tangent} has the equation of
                $$
                    y - f(x_0) = f'(x_0) (x - x_0),
                $$
                and the \textbf{normal} has the equation of
                $$
                    y - f(x_0) = - \frac{1}{f'(x_0)} (x - x_0).
                $$
            \end{definition}

            \begin{theorem}[small increments]
                For two points $P(x, y)$ and $Q(x + \delta x, y + \delta y)$ on the curve, if $\delta x$ is sufficiently small (i.e. $P$ and $Q$ are sufficiently close), then we have
                $$
                    \frac{\delta y}{\delta x} \approx \frac{\diff y}{\diff x}.
                $$
            \end{theorem}

            \begin{theorem}[rate of change]
                When we do those rate of change questions, note that the rate of change of variable $v$ will be
                $$
                    \frac{\diff}{\diff t} v,
                $$
                and we may use the chain rule and the rule that
                $$
                    \frac{\diff x}{\diff y} = \frac{1}{\frac{\diff y}{\diff x}}.
                $$
            \end{theorem}

            \begin{theorem}[differentiation of polynomials/power functions]
                The power function $y = x^n$ satisfies that
                $$
                    \frac{\diff y}{\diff x} = n x ^ {n - 1}.
                $$
            \end{theorem}

            \begin{theorem}[differentiation of trig functions]
                We have
                $$
                    \frac{\diff}{\diff x} \sin x = \cos x,
                $$
                $$
                    \frac{\diff}{\diff x} \cos x = - \sin x,
                $$
                $$
                    \frac{\diff}{\diff x} \tan x = \sec^2 x.
                $$
            \end{theorem}

            \begin{theorem}[differentiation of exponential and logarithmic]
                We have
                $$
                    \frac{\diff}{\diff x} e^x = e^x,
                $$
                $$
                    \frac{\diff}{\diff x} \ln x = \frac{1}{x}.
                $$
            \end{theorem}

            \begin{definition}[second derivative]
                The \textbf{second derivative} is obtained by differentiating the first derivative. We denote it as
                $$
                    f''(x), \frac{\diff^2}{\diff x^2} f(x), \frac{\diff^2 f(x)}{\diff x^2}, \frac{\diff}{\diff x} \frac{\diff f(x)}{\diff x}.
                $$
            \end{definition}

            From now on, we will consider derivatives and the graph of the function. To make things easier, we will assume that the function $f(x)$ is differentiable.

            \begin{definition}[stationary point, turning point]
                The \textbf{stationary point} or \textbf{turning point} is a point where the gradient is zero.
            \end{definition}

            \begin{definition}[maximum point, minimum point]
                A point $(x_0, f(x_0))$ is a \textbf{maximum point} if for a certain neighbourhood $(x_0 - \delta, x_0 + \delta)$ within its domain we have
                $$
                    \forall x \in (x_0 - \delta, x_0 + \delta) : f(x) \leq f(x_0).
                $$

                A minimum point is defined similarly.
            \end{definition}

            \begin{lemma}[maximum/minimum and stationary point]
                A maximum point/minimum point must be a stationary point.
            \end{lemma}

            \begin{theorem}[maximum, minimum, point of inflextion]
                For a point $P(x_0, f(x_0))$, it is a \textbf{maximum point} if and only if (in our discussion domain):
                $$
                    f'(x_0) = 0,
                $$
                and $f'(x)$ turns from positive to negative at the point $x = x_0$.

                The condition of a minimum point is defined similarly.

                If a point is a stationary point but is not a maximum or a minimum, we call it a \textbf{point of inflextion}. It must satisfy that $f'(x)$ briefly 'touches' $0$ from positive to positive or from negative to negative.
            \end{theorem}

            \begin{theorem}[second derivative test]
                If for a stationary point $P(x_0, f(x_0))$, consider the second derivative at that point,
                \begin{itemize}
                    \item If $f''(x_0) > 0$, it is a minimum;
                    \item if $f''(x_0) < 0$, it is a maximum;
                    \item if $f''(x_0) = 0$, it is indeterminable by this method.
                \end{itemize}
                
                Notice that one can use this method if and only if the point investigating is a stationary point. 
            \end{theorem}

        \subsection{Integration}
            \begin{definition}[indefinite integral]
                If we have a function $f(x)$, and we have a function $F(x)$ s.t. $F'(x) = f(x)$, we say $F(x)$ is an \textbf{antiderivative} of the function $f(x)$. Without proof, we say that $F(x)$ can form a class of functions with a difference of a constant. Hence we can define the \textbf{indefinite integral} as
                $$
                    \int f(x) \diff x = F(x) + C,
                $$
                where $C$ is called the \textbf{integration constaant}.
            \end{definition}

            \begin{theorem}[linearality of indefinite integrals]
                For two functions $f(x)$, $g(x)$ and a constant $k$, we have
                $$
                    \int (f(x) + g(x)) \diff x = \int f(x) \diff x + \int g(x) \diff x,
                $$
                $$
                    \int k f(x) \diff x = k \int f(x) \diff x.
                $$
            \end{theorem}

            \begin{theorem}[integration of polynomials and powers]
                For a function $y = x^n$ where $n \neq -1$, we have
                $$
                    \int x^n \diff x = \frac{1}{n + 1} x^{n + 1} + C.
                $$

                We also have
                $$
                    \int \frac{1}{x} \diff x = \ln \abs{x} + C.
                $$
            \end{theorem}

            \begin{theorem}[integration of exponential]
                We have
                $$
                    \int e^x \diff x = e^x + C.
                $$
            \end{theorem}

            \begin{theorem}[integration of trigs]
                We have
                $$
                    \int \sin x \diff x = - \cos x + C,
                $$
                $$
                    \int \cos x \diff x = \sin x + C.
                $$
            \end{theorem}

            \begin{theorem}[substitution in integration]
                We have
                $$
                    \int f(g(x)) \cdot g'(x) \diff x = \int f(g(x)) \diff g(x),
                $$
                and if $g'(x)$ is a constant, we can have
                $$
                    \int f(g(x)) \diff x = \frac{1}{g'(x)} \int f(g(x)) \diff g(x)
                $$

                Using this we can find the composite of previous functions with $ax + b$.
            \end{theorem}

            \begin{definition}[definite integral, Riemann integral]
                The \textbf{definite integral}, in this case the \textbf{Riemann integral} of a function $f(x)$ from lower limit $a$ to upper limit $b$, is defined as the area below the graph of $f(x)$ from $a$ to $b$, is denoted as
                $$
                    \int_{a}^{b} f(x) \diff x.
                $$
            \end{definition}
            
            \begin{theorem}[Newton--Leibniz Formula, fundamental theorem of calculus]
                We have
                $$
                    \int_a^b f(x) \diff x = \left[F(x)\right]_a^b = F(b) - F(a),
                $$
                where $F(x)$ is an antiderivative of $f(x)$.
                \begin{proof}
                    A basic explanation is: when we 'sum' together some small increments (significance of Riemann Integral) which are derivatives of the original function, we in fact just get the increment in the original function itself!
                \end{proof}
            \end{theorem}

            \begin{definition}[kinematics]
                For displacement $s$, velocity $v$, acceleration $a$, time $t$, we have
                $$
                    v = \dot{s} = \frac{\diff s}{\diff t}, a = \dot{v} = \frac{\diff v}{\diff t} = \ddot{s} = \frac{\diff^2 s}{\diff t^2}.
                $$
            \end{definition}

            \begin{corollary}[integration point of view]
                We have
                $$
                    v = \int a \diff t, s = \int v \diff t.
                $$
            \end{corollary}

            \begin{definition}[kinematics word description]
                If $s > 0$, we can say the object is to the right of $O$. If $s = 0$, we can say the object is at $O$. If $s < 0$, we can say the object is to the left of $O$.

                If $v > 0$, we can say the object is moving to the right. If $v = 0$, we can say the object is at rest (instantaneously). If $v < 0$, we can say the object is moving to the left.

                If $a > 0$, we can say the velocity is increasing. If $a = 0$, we can say velocity can be maximum, minimum, or constant. If $a < 0$, we can say velocity is decreasing.
            \end{definition}

    \section{Vectors in two dimensions}

        \begin{definition}[vector, scalar]
            A \textbf{vector} is an element of a linear space that a scalar multiplication and addition can be defined on, and a \textbf{scalar} is an element of a field. Just kidding.

            A \textbf{vector} is a quantity that has both magnitude and direction. A \textbf{scalar} is a quantity which only has a magnitude.
        \end{definition}

        \begin{definition}[notation of vectors]
            A vector can be represented as a bold letter $\vect{a}$. In handwriting, we denote it as $\underaccent{\tilde}{a}$.

            A vector from point $A$ to point $B$ is denoted as $\ray{AB}$.

            The vector going right $x$ units and going up $y$ units can be denoted as
            $$
                \binom{x}{y}
            $$
            or
            $$
                x\vect{i} + y\vect{j},
            $$
            where $\vect{i}$ and $\vect{j}$ is the unit vectors in the $x$ direction and $y$ direction.
        \end{definition}

        \begin{definition}[modulus]
            A \textbf{modulus} (I will prefer norm instead) of a vector $\vect{a}$ represented as $x\vect{i} + y\vect{j}$ will be equal to
            $$
                \abs{\vect{a}} = \sqrt{x^2 + y^2}.
            $$

            It is also the \textbf{length} of the vector $\vect{a}$.
        \end{definition}

        \begin{definition}[unit vector]
            A \textbf{unit vector} is a vector which has a length of $1$ in a certain direction.
        \end{definition}

        \begin{definition}[inverse]
            By the axioms of a linear space, a vector always has another vector which summed together gets the addition unit (0), and we denote the inverse of a vector $\vect{a}$ as $-\vect{a}$ since it exists and is unique. Just kidding.

            We denote $-\vect{a}$ as the vector which has the same length but the opposite direction of $\vect{a}$.

            In particular, if $\vect{a} = x\vect{i} + y\vect{j}$, then $-\vect{a} = -x\vect{i} -y\vect{j}$.
        \end{definition}

        \begin{definition}[addition]
            We denote $\vect{a} + \vect{b}$ as the sum of two vectors and $\vect{a} - \vect{b} = \vect{a} + (-\vect{b})$.

            In particular, for $\vect{a} = x_a\vect{i} + y_a\vect{j}$, and for $\vect{b} = x_b \vect{i} + y_b \vect{j}$, we have
            $$
                \vect{a} + \vect{b} = (x_a + x_b) \vect{i} + (y_a + y_b) \vect{j},
            $$
            and
            $$
                \vect{a} - \vect{b} = (x_a - x_b) \vect{i} + (y_a - y_b) \vect{j}.
            $$
        \end{definition}

        \begin{definition}[scalar multiplication]
            We denote $k\vect{a}$ as the product of the scalar $k$ with the vector $\vect{a}$.

            In particular, for $\vect{a} = x \vect{i} + y \vect{j}$, we have
            $$
                k\vect{a} = (kx) \vect{i} + (ky) \vect{j}.
            $$
        \end{definition}

        \begin{definition}[colinear points]
            If for points $A, B, C$ we have a scalar $k$ s.t.
            $$
                \ray{AB} = k \ray{AC},
            $$
            then we say $A, B, C$ are colinear.
        \end{definition}

        \begin{definition}[position vector]
            The position vector of vector $P$ is the vector $\ray{OP}$ where $O$ is the origin.
        \end{definition}

        \begin{theorem}[addition and subtraction rule]
            We have
            $$
                \ray{AB} + \ray{BC} = \ray{AC},
            $$
            and
            $$
                \ray{AB} = \ray{CB} - \ray{CA}.
            $$
        \end{theorem}

    \section{Permutations and combinations}

        \begin{definition}[factorial]
            We define $n!$ as the product of numbers from $1$ to $n$. An inductive definition of it will be
            $$
                n! = n \cdot (n-1)!,
            $$
            with the definition of $0! = 0$.
        \end{definition}

        \begin{theorem}[arrangement, all, line]
            The number of ways of arranging $n$ distinct items in a line equals $n!$.
            \begin{proof}
                Proof by induction. This is true for $1$ items since $1! = 1$. Assume arranging $(n - 1)$ items in a line has $(n - 1)!$. Then choosing the first item has $n$ choices and arranging the rest $(n - 1)$ has $(n - 1)!$ choices. So in total, we have $n! = n \cdot (n - 1)!$ choices. 
            \end{proof}
        \end{theorem}

        \begin{theorem}[permutation, some]
            The number of permutations of $r$ items from $n$ distinct items is
            $$
                ^nP_r = \frac{n!}{(n-r)!} = n \cdot (n-1) \cdots (n - r + 1).
            $$
            \begin{proof}
                Choosing the first object in the $r$-length permutation will have $n$ choices. The second one will have $(n - 1)$ choices. This extends to the $r$th object which has $(n - r + 1)$ choices.
            \end{proof}
        \end{theorem}

        \begin{remark}
            Order matters in permutations/arrangements.
        \end{remark}

        \begin{theorem}[combination, some]
            The number of combinations of $r$ items from $n$ distinct items is
            $$
                ^nC_r = \binom{n}{r} = \frac{n!}{r! (n-r)!}.
            $$
            \begin{proof}
                This is because we counted $r!$ for the same objects in different permutations.
            \end{proof}
        \end{theorem}

        \begin{remark}
            The order does not matter in combinations.
        \end{remark}

        \begin{theorem}[arrangement, repeating]
            Say there are $a_1$ same objects of type $1$, $a_2$ same objects of type $2$, all the way to $a_k$ same objects of type $k$. Say the total number of objects is $n$. Then the total number of arrangements will be
            $$
                \frac{n!}{a_1! a_2! \cdots a_k!}.
            $$
            \begin{proof}
                We now say the objects are different. Then we have $n!$ choices. Each 'different object arrangement' within a certain type is counted as $a_1!, a_2!, \ldots, a_k!$ types so we divide them by this.
            \end{proof}
            \begin{proof}
                We can think of this as a combinatoric argument. The number of choices will be
                $$
                    \binom{n}{a_1} \cdot \binom{n - a_1}{a_2} \cdot \binom{n - a_1 - a_2}{a_3} \cdots \binom{n - a_1 - a_2 \cdots - a_{k - 1}}{a_k}.
                $$
            \end{proof}
        \end{theorem}

        \begin{theorem}[permutation, repeating]
            Say there are $r$ choices for each of the $n$ positions (e.g. password). The number of total permutations will be $r^n$.
        \end{theorem}

    \section{Series}

        \begin{theorem}[binominal theorem]
            We have
            $$
                (a + b)^n = a^n + \binom{n}{1} a^{n - 1}b + \binom{n}{2} a^{n - 2}b^2 + \cdots + \binom{n}{n - 1} a b^{n - 1} + \binom{n}{n} b^n.
            $$
            
            This can also be expressed as
            $$
                (a + b)^n = \sum_{i = 0}^{n} \binom{n}{i} a^{n - i} b^i.
            $$

            \begin{proof}
                We can prove this by induction. Notice that
                $$
                    \binom{n}{r} = \binom{n - 1}{r - 1} + \binom{n - 1}{r},
                $$
                which also represents Pascal's triangle.
            \end{proof}
        \end{theorem}

        \begin{remark}
            This can be generalised to all rational $n$s (hence all real $n$s).
        \end{remark}
        
        \begin{corollary}[term in a binominal expansion]
            In a binominal expansion, the general term will be
            $$
                \binom{n}{r} a^r b^{n - r}.
            $$
        \end{corollary}

        \begin{definition}[arithmetic progression/sequence]
            An \textbf{arithmetic progression/sequence} is when the next term is the previous term plus a certain difference which we call the \textbf{common difference}. In fact, we say the following:
            $$
                a_n = a_{n - 1} + d.
            $$
        \end{definition}

        \begin{theorem}[$n$th term expression]
            By basic induction, we can have
            $$
                a_n = a_1 + (n - 1)d.
            $$
        \end{theorem}

        \begin{theorem}[sum to the $n$th term]
            We have
            $$
                s_n = \frac{n(a_1 + a_n)}{2} = \frac{n [2a_1 + (n - 1)d]}{2}.
            $$
            \begin{proof}
                We can also prove this by induction. Notice that
                $$
                    a_n = s_n = s_{n - 1}.
                $$
            \end{proof}
        \end{theorem}

        \begin{definition}[geometric progression/sequence]
            A \textbf{geometric progression/sequence} is when the next term is the previous term times a certain ratio which we call the \textbf{common ratio}. In fact, we say the following:
            $$
                g_n = g_{n - 1} \cdot r.
            $$
        \end{definition}

        \begin{theorem}[$n$th term expression]
            By basic induction, we can have
            $$
                g_n = g_1 \cdot r^{n - 1}.
            $$
        \end{theorem}

        \begin{theorem}[sum to the $n$th term]
            We have
            $$
                s_n = \frac{a (1 - r^n)}{1 - r}.
            $$
            \begin{proof}
                We time $r$ to $s_n$ and minus it by the original $s_n$ to eliminate most terms.
            \end{proof}
        \end{theorem}

        \begin{theorem}[sum to infinity]
            We have
            $$
                s_\infty = \lim_{n \rightarrow \infty} s_n = \frac{a}{1 - r}.
            $$
            This limit exists (and has the value above) if and only if $\abs{r}<1$.
        \end{theorem}

    \section*{Afterwords}

        This sheet took me some time to populate, but I would genuinely like to share my understanding of GCSE Additional Maths (and beyond) with all of you.
            
        My deepest thanks to all the Maths teachers I have met and my friends who helped and supported me with producing this.
        
        I would like to give special thanks to Mr Finch-Noyes, my current Maths teacher, and Dr Zhang, Ms Zhu and Mr Feng, my tutor for providing me with all this knowledge.

        Finally, I hope this helped you to gain a better understanding of Maths. Feel free to email \href{eason.syc@icloud.com}{eason.syc@icloud.com} to send me feedback.


\end{document}